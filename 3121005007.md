## Python实现论文查重+性能检测

### 需要调用的库函数
```import re
import jieba
from simhash import Simhash
import time
from memory_profiler import profile
```

### 查重函数
```# 查重函数，接口为初始文本和抄袭文本

def check_similarity(original_text, copy_text):
    # 创建了一个正则表达式模式
    regex = re.compile(r'[\u4e00-\u9fa5]+')
    # 删除标点
    cl_test1 = re.findall(regex, original_text)
    cl_test2 = re.findall(regex, copy_text)
    # 将删除标点后的文本以字符串的形式存储
    original_text = ''.join(cl_test1)
    copy_text = ''.join(cl_test2)

    # 对字符串进行分词
    original_words = list(jieba.cut(original_text))
    copy_words = list(jieba.cut(copy_text))

    # 生成simhash值
    original_simhash = Simhash(original_words)
    copy_simhash = Simhash(copy_words)
    # 计算海明距离
    distance = original_simhash.distance(copy_simhash)
    # 计算查重率
    similarity = 1 - distance / 64
    return similarity
```
### 主函数
```# 输入文件
original_text = input('输入原文文件路径：')
copy_text = input('输入抄袭版论文文件路径：')
result = input('输入答案文件保存路径：')
# 读取原始文件和抄袭文件
with open(original_text, 'r', encoding='utf-8') as f1:
    text1 = f1.read()
with open(copy_text, 'r', encoding='utf-8') as f2:
    text2 = f2.read()

similarity = check_similarity(text1, text2)
# 输出结果到指定文本中
with open(result, 'w', encoding='utf-8') as f3:
    # 以百分比形式呈现到文本中
    f3.write('抄袭版论文与原文的重复率为：{:.2f}%'.format(similarity * 100))
    f3.write('\n')
    f3.write('程序执行时间：{:.2f}秒'.format(end_time - start_time))

print('抄袭版论文与原文的重复率为：{:.2f}%'.format(similarity * 100))
print('程序执行时间：{:.2f}秒'.format(end_time - start_time))
```
### 性能检测
```
@profile # 这一部分使用在查重函数部分，重点查看查重函数性能运行情况
# 开始性能和内存监测
start_time = time.time()
# 结束性能和内存监测
end_time = time.time()
```
###最终性能分析结果
![image](https://github.com/spottern/JianPeng-Su/assets/126228869/4ec4f482-f0d1-4dae-8811-5783265a017c)

